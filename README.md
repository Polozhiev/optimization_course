# optimization_course

## lab 1:

- Gradient descent 
- Line search with Armijo and Wolfe conditions
- Formulas for the gradient and Hessian of the logistic regression function are obtained
- Newton's method
- Some [experiments](https://github.com/Polozhiev/optimization_course/blob/main/task1/lab1_Polozhiev.pdf) are done

## lab 2:
- Conjugate gradient method
- Hessian-free Newton's method
- L-BFGS method
- Some [experiments](https://github.com/Polozhiev/optimization_course/blob/main/task2/lab2_Polozhiev.pdf) are done